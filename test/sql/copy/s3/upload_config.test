# name: test/sql/copy/s3/upload_config.test
# description: Various working and non-working configs for the uploader
# group: [s3]

require tpch

require parquet

require httpfs

require-env S3_TEST_SERVER_AVAILABLE 1

# override the default behaviour of skipping HTTP errors and connection failures: this test fails on connection issues
set ignore_error_messages

statement ok
SET s3_secret_access_key='S3RVER';SET s3_access_key_id='S3RVER';SET s3_region='eu-west-1'; SET s3_endpoint='http://localhost:4923';

statement ok
CALL DBGEN(sf=1)

# First try a valid config: 5TB in 10000 parts will have a fairly reasonably 500 MB per part
statement ok
SET s3_uploader_max_parts_per_file=10000;

statement ok
SET s3_uploader_max_filesize='5TB';

statement ok
SET s3_uploader_thread_limit = 2;

statement ok
COPY lineitem TO 's3://test-bucket/multipart/export_out_largeparts.csv' WITH (HEADER 1, DELIMITER '|');

query I
SELECT
    sum(l_extendedprice * l_discount) AS revenue
FROM
    "s3://test-bucket/multipart/export_out_largeparts.csv"
WHERE
    l_shipdate >= CAST('1994-01-01' AS date)
    AND l_shipdate < CAST('1995-01-01' AS date)
    AND l_discount BETWEEN 0.05
    AND 0.07
    AND l_quantity < 24;
----
123141078.228299

# Now set an invalid config, there should not be enough memory to support this config as a single part would require 2GB of memory
statement ok
SET memory_limit='1GB';

statement ok
SET s3_uploader_max_parts_per_file=1000;

statement ok
SET s3_uploader_max_filesize='2TB';

statement error
COPY lineitem TO 's3://test-bucket/multipart/export_out_should_not_exist.csv' WITH (HEADER 1, DELIMITER '|');


statement ok
PRAGMA TPCH(1);

statement ok
PRAGMA TPCH(2);

statement ok
PRAGMA TPCH(3);

statement ok
PRAGMA TPCH(4);

# This should work barely: part size is 100MB, memory limit is 300MB, this will cause allocation to fail requiring the
# uploader to wait for memory to become available
# Only be able to do 1 buffer at a time.
statement ok
SET memory_limit='300MB';

statement ok
SET s3_uploader_max_parts_per_file=10000;

statement ok
SET s3_uploader_max_filesize='1TB';

statement ok
SET s3_uploader_thread_limit=10;

statement ok
COPY lineitem TO 's3://test-bucket/multipart/export_out_should_not_exist.csv' WITH (HEADER 1, DELIMITER '|');


# This should work barely: part size is 100MB, memory limit is 300MB, this will cause allocation to fail requiring the
# uploader to wait for memory to become available
# Only be able to do 1 buffer at a time.
statement ok
SET memory_limit='1GB';

statement ok
SET s3_uploader_max_parts_per_file=10000;

statement ok
SET s3_uploader_max_filesize='1TB';

statement ok
SET s3_uploader_thread_limit=0;

statement ok
COPY lineitem TO 's3://test-bucket/multipart/export_out_should_not_exist.csv' WITH (HEADER 1, DELIMITER '|');